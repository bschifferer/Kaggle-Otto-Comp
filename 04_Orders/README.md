# Orders Submission

The code for clicks, carts and orders are similar. However, during the competition, I developed slighly different variation. Providing a full and accurate pipeline, I provide them as separate pipelines. 

**Important: The pipelines (clicks, carts and orders) will write into the same directory. Do not run them in parallel**

## Files Inventory

Execute the notebooks in the order indicated by the name.

It is not required to run the notebooks multiple times for different `igfold` values.

```
├── 01a_Split-clicks.ipynb                      # Generates candidates+add feature scores for clicks
├── 01b_Split-carts-orders-sub.ipynb            # Generates candidates+add feature scores for carts and orders
├── 02_Split_2_Add_Emb_Features-v2.ipynb        # Adds similarity scores based on Word2Vec embeddings to it
├── 03_XGB_Clicks.ipynb                         # Trains XGBoost model to predict click target for carts/orders and submission input
├── 04_XGB_Carts.ipynb                          # Trains XGBoost model to predict carts target for orders and submission input
├── 05a_XGB-orders-localCV.ipynb                # Trains XGBoost model to get local CV score
├── 05b_XGB_orders-submission.ipynb             # Trains XGBoost model to predict submission dataset
├── 06_Combine-Bags.ipynb                       # Reads the predicted folds/bags and generates a submission.csv
```

## Additional coomments

#### Main Differences to Click and Cart pipeline

The order pipeline is more complex and has more files. The XGBoost input for orders contain the XGBoost prediction from cart and from clicks. (The XGBoost input from cart contains XGBoost prediction from clicks). The pipeline generated 'out-of-fold' predictions. The click dataset is larger than carts and orders, therefore, the pipeline filters all sessions, which contains a positive click target BUT does not have a carts or order target.
```
session_filter = labels[(labels['type']=='carts')|(labels['type']=='orders')]['session'].drop_duplicates().to_pandas().tolist()
```
Simlar Carts -> orders
```
session_filter = labels[(labels['type']=='orders')]['session'].drop_duplicates().to_pandas().tolist()
```

These session_ids will be removed before training the XGBoost model in `03_XGB_Clicks.ipynb` and `04_XGB_Carts.ipynb`.

- 01_Split - Candidate Generation: I noticed that increasing the number of candidates improved local CV and LB. The clicks dataset is too large to push the number of candidates futher. Therefore, I increased the number of candidates only for carts and orders. I changed the logic to select candidates, if a candidate was generated by at least 2 different logics (e.g. two different co-visitation matrices)


#### 01a_Split-clicks.ipynb 

This file should be equivalent to `../02_Clicks/01_Split.ipynb`

#### 01b_Split-carts-orders-sub.ipynb

In this notebook following steps will be executed:
- Previous input files are splitted by session ids to reduce memory footprint (`glob.glob('./data_folds/fold_' + str(igfold) + '/candidates/train/*/cand.parquet')`and `glob.glob('./data_folds/fold_' + str(igfold) + '/candidates/sub/*/cand.parquet')`
- Candidates per session_id is generated in `get_cands` function
-- The top80 candidates will be kept **for any input file**. Afterwards, only candidates are selected, which were recommended by at least two different logics 
-- The scores from all input files will be added to each session x item pairs
- Additional session features will be added 
- Additional candidate features will be added
- Target column (local CV) will be added (only for train)
- Note: carts and orders targets will be exectued

The pipeline will be executed for train and submission dataframe and for all folds.

#### 02_Split_2_Add_Emb_Features-v2.ipynb

In this notebook, we will add the similarity scores of Word2Vec embeddings for the train and submission dataset (+ all folds).
The notebook will overwrite the output from `01_Split.ipynb ` to save disk space. It will iterate over clicks, carts, orders and submissions.

#### 03_XGB_Clicks.ipynb

This notebook will train the XGBoost model based on the output from `02_Split_2_Add_Emb_Features-v2.ipynb`. It will be similar to `../02_Clicks/03_XGB-v19-clicks.ipynb`. The difference is, that it filters out sessions with carts and orders targets. It adds the click scores to the input dataframe and stores it in another folder. 

#### 04_XGB_Carts.ipynb

This notebook will train the XGBoost model based on the output from `02_Split_2_Add_Emb_Features-v2.ipynb`. It will be similar to `../03_Carts/03_XGB-v20-carts.ipynb`. The difference is, that it filters out sessions with orders targets. It adds the carts scores to the input dataframe and stores it in another folder. 

#### 05a_XGB-orders-localCV & 05b_XGB_orders-submission

This notebook will train the XGBoost model based on the output from `04_XGB_Carts.ipynb`. It does not use `bl_sub = False` and the difference between local CV and submissions are provided as separate notebooks.

#### 06_Combine-Bags.ipynb 

This notebook will combine the different folds and bags for a final submission file. The output will be stored in `../00_submissions/24_XGB_Rerun_RedruceCandidate_DifferentWeights_Folds_ChrisCo_SameDay_v9`. It will load the `submission.csv` from clicks and carts and generates the final submission.csv.